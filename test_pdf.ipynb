{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import bs4\n",
    "import chromadb.config\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        uploaded_files,\n",
    "    ):\n",
    "    TEXT = \"\"\n",
    "\n",
    "    # for file in uploaded_files:\n",
    "    with open(uploaded_files, 'rb') as f:\n",
    "        # if file.type == \"application/pdf\":\n",
    "        pdf_reader = PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        TEXT += text\n",
    "        docs = TEXT\n",
    "    return docs\n",
    "\n",
    "def embbedings_and_store(docs):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    persist_directory = \"chroma_db\"\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=5000, chunk_overlap=1000, length_function=len)\n",
    "    text_chunks = text_splitter.split_text(docs)\n",
    "    \n",
    "    vectorstore = Chroma.from_texts(text_chunks, embedding=embeddings, persist_directory=persist_directory)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data(uploaded_files='gobook.pdf')\n",
    "vector = embbedings_and_store(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff6546cda50>, search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_chain(retriever):\n",
    "    ### Contextualize question ###\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    store = {}\n",
    "    \n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(vector_embeddings):\n",
    "    # llm = ChatOpenAI()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True\n",
    "    )\n",
    "    conversation = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_embeddings,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBranch(branches=[(RunnableBinding(bound=RunnableLambda(_is_not_async), config={'run_name': 'RunnableWithMessageHistoryInAsyncMode'}), RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff6546cda50>, search_kwargs={'k': 2}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff658567640>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff65858cd90>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff6546cda50>, search_kwargs={'k': 2})), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff658567640>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff65858cd90>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}), config_factories=[<function Runnable.with_alisteners.<locals>.<lambda> at 0x7ff657484310>]))], default=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "             | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff6546cda50>, search_kwargs={'k': 2}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff658567640>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff65858cd90>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "             | StrOutputParser()\n",
       "             | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7ff6546cda50>, search_kwargs={'k': 2})), config={'run_name': 'retrieve_documents'})\n",
       "  })\n",
       "  | RunnableAssign(mapper={\n",
       "      answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                context: RunnableLambda(format_docs)\n",
       "              }), config={'run_name': 'format_inputs'})\n",
       "              | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ff658567640>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff65858cd90>, model_name='gpt-3.5-turbo-0125', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "              | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "    }), config={'run_name': 'retrieval_chain'}, config_factories=[<function RunnableBinding.with_listeners.<locals>.<lambda> at 0x7ff6574843a0>])), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function create_conversational_chain.<locals>.get_session_history at 0x7ff65857b7f0>, input_messages_key='input', output_messages_key='answer', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = create_conversational_chain(vector)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_chat(query, chain, history=None):\n",
    "    result = chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # constructs a key \"abc123\" in `store`.\n",
    "    )[\"answer\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(query_input, file, history = None):\n",
    "    \n",
    "    # data_loader = load_data(\n",
    "    #     'dataset/imdb_top_1000.csv'\n",
    "    # )\n",
    "    vectorstores = embbedings_and_store(file)\n",
    "    chain = create_conversational_chain(vectorstores)\n",
    "    result = conversation_chat(\n",
    "        query_input, \n",
    "        chain=chain, \n",
    "        history=history\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Go, class-based programming is not supported. However, you can achieve similar functionality using struct types and methods. Structs in Go can have fields and methods associated with them, allowing you to encapsulate data and behavior together. By defining methods on structs, you can create behavior similar to class methods in other languages.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chat(\n",
    "    query=\"tell me just a brief, while class based programming not supported on go, what can we do while we want to make a class ?\",\n",
    "    chain=conv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Go, you can create a struct with fields and methods to achieve similar functionality to class-based programming. Here is a simple example:\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\ntype Person struct {\\n    Name string\\n    Age  int\\n}\\n\\nfunc (p Person) greet() {\\n    fmt.Printf(\"Hello, my name is %s and I am %d years old.\\\\n\", p.Name, p.Age)\\n}\\n\\nfunc main() {\\n    p := Person{Name: \"Alice\", Age: 30}\\n    p.greet()\\n}\\n```\\n\\nIn this example, we define a `Person` struct with `Name` and `Age` fields. We then define a `greet` method associated with the `Person` struct that prints a greeting message. Finally, in the `main` function, we create an instance of `Person` and call the `greet` method on it.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chat(\n",
    "    query=\"can you show me how create that ?\",\n",
    "    chain=conv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell me just a brief, while class based programming not supported on go, what can we do while we want to make a class ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m result\n",
      "File \u001b[0;32m~/project/llamaindex/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:4590\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4585\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4586\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4587\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m   4588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4589\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m-> 4590\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   4591\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4592\u001b[0m     )\n",
      "File \u001b[0;32m~/project/llamaindex/.venv/lib/python3.10/site-packages/langchain_core/runnables/history.py:533\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory._merge_configs\u001b[0;34m(self, *configs)\u001b[0m\n\u001b[1;32m    529\u001b[0m     example_configurable \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    530\u001b[0m         missing_key: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[your-value-here]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m missing_key \u001b[38;5;129;01min\u001b[39;00m missing_keys\n\u001b[1;32m    531\u001b[0m     }\n\u001b[1;32m    532\u001b[0m     example_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_configurable}\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(missing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in config[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected keys are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(expected_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using via .invoke() or .stream(), pass in a config; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g., chain.invoke(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    540\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m _get_parameter_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_history)\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expected_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# If arity = 1, then invoke function by positional arguments\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing keys ['session_id'] in config['configurable'] Expected keys are ['session_id'].When using via .invoke() or .stream(), pass in a config; e.g., chain.invoke({'input': 'foo'}, {'configurable': {'session_id': '[your-value-here]'}})"
     ]
    }
   ],
   "source": [
    "result = conv.invoke(\n",
    "    {\n",
    "        \"question\":\"tell me just a brief, while class based programming not supported on go, what can we do while we want to make a class ?\"\n",
    "    }\n",
    ")[\"answer\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Go, you can create a class-like structure using structs and methods. While Go does not have classes like other object-oriented languages, you can achieve similar behavior by defining methods on structs. Here is a simple example:\\n\\n```go\\npackage main\\n\\nimport \"fmt\"\\n\\ntype Person struct {\\n    Name string\\n    Age  int\\n}\\n\\nfunc (p Person) SayHello() {\\n    fmt.Printf(\"Hello, my name is %s and I am %d years old.\\\\n\", p.Name, p.Age)\\n}\\n\\nfunc main() {\\n    p := Person{Name: \"Alice\", Age: 30}\\n    p.SayHello()\\n}\\n```\\n\\nIn this example, we define a `Person` struct with `Name` and `Age` fields. We then define a method `SayHello` on the `Person` struct, which allows instances of `Person` to call `SayHello()` to print a greeting with their name and age.\\n\\nBy attaching methods to structs, you can achieve similar functionality to classes in other languages.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = conv.invoke(\n",
    "    {\n",
    "        \"question\":\"can you show me how create that ?\"\n",
    "    }\n",
    ")[\"answer\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
