{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import bs4\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        uploaded_files,\n",
    "        # dataset_name = 'imdb_top_1000.csv'\n",
    "    ):\n",
    "    TEXT = \"\"\n",
    "\n",
    "    # for file in uploaded_files:\n",
    "    with open(uploaded_files, 'rb') as f:\n",
    "        # if file.type == \"application/pdf\":\n",
    "        pdf_reader = PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        TEXT += text\n",
    "        docs = TEXT\n",
    "    return docs\n",
    "\n",
    "def embbedings_and_store(docs):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    persist_directory = \"chroma_db\"\n",
    "    \n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    # splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=5000, chunk_overlap=1000, length_function=len)\n",
    "    text_chunks = text_splitter.split_text(docs)\n",
    "    \n",
    "    vectorstore = Chroma.from_texts(text_chunks, embedding=embeddings, persist_directory=persist_directory)\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        # search_kwargs={\"k\": 2}\n",
    "    )\n",
    "    # db = Chroma.from_documents(\n",
    "    #         documents=docs, embedding=embeddings, persist_directory=persist_directory\n",
    "    # )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data(uploaded_files='gobook.pdf')\n",
    "vector = embbedings_and_store(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7fd2cd57f220>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_chain(retriever):\n",
    "    ### Contextualize question ###\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    store = {}\n",
    "    \n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(vector_embeddings):\n",
    "    llm = ChatOpenAI()\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True\n",
    "    )\n",
    "    conversation = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_embeddings,\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferMemory(return_messages=True, memory_key='chat_history'), combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fd2cd92a6e0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd2cdaf45e0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fd2cd92a6e0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd2cdaf45e0>, openai_api_key=SecretStr('**********'), openai_proxy='')), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7fd2cd57f220>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = start_conversation(vector)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_chat(query, chain, history=None):\n",
    "    result = chain.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # constructs a key \"abc123\" in `store`.\n",
    "    )[\"answer\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(query_input, file, history = None):\n",
    "    \n",
    "    # data_loader = load_data(\n",
    "    #     'dataset/imdb_top_1000.csv'\n",
    "    # )\n",
    "    vectorstores = embbedings_and_store(file)\n",
    "    chain = create_conversational_chain(vectorstores)\n",
    "    result = conversation_chat(\n",
    "        query_input, \n",
    "        chain=chain, \n",
    "        history=history\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Go, which does not support traditional class-based programming, you can achieve similar functionality using structs and methods. By defining a struct to represent data and attaching methods to that struct, you can create a concept similar to classes in other languages. This approach allows you to encapsulate data and behavior together, providing a way to organize and interact with your code effectively.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = conv.invoke(\n",
    "    {\n",
    "        \"question\":\"tell me just a brief, while class based programming not supported on go, what can we do while we want to make a class ?\"\n",
    "    }\n",
    ")[\"answer\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Go, you can create a class-like structure using structs and methods. Here\\'s an example to demonstrate how you can achieve this:\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\"fmt\"\\n)\\n\\n// Define a struct to represent a person\\ntype Person struct {\\n\\tName string\\n\\tAge  int\\n}\\n\\n// Define a method for the Person struct to introduce the person\\nfunc (p Person) Introduce() {\\n\\tfmt.Printf(\"Hi, my name is %s and I am %d years old.\\\\n\", p.Name, p.Age)\\n}\\n\\nfunc main() {\\n\\t// Create an instance of the Person struct\\n\\tp := Person{Name: \"Alice\", Age: 30}\\n\\n\\t// Call the Introduce method on the Person instance\\n\\tp.Introduce()\\n}\\n```\\n\\nIn this example:\\n- We define a struct `Person` with fields `Name` and `Age`.\\n- We define a method `Introduce` for the `Person` struct that prints out an introduction.\\n- We create an instance `p` of the `Person` struct with a name \"Alice\" and age 30.\\n- We call the `Introduce` method on the `p` instance to introduce the person.\\n\\nThis demonstrates how you can create a class-like structure in Go using structs and methods.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = conv.invoke(\n",
    "    {\n",
    "        \"question\":\"can you show me how create that ?\"\n",
    "    }\n",
    ")[\"answer\"]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
